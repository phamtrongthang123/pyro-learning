{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyro-3-kind-finding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGU9gta_3v-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install pyro-ppl \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyAQ02Dajes3",
        "colab_type": "text"
      },
      "source": [
        "The example below I use from: https://pyro.ai/examples/svi_part_i.html#A-simple-example\n",
        "\n",
        "We will start with the quote from: https://forum.pyro.ai/t/correct-map-guide-without-using-automatic-guide-generation/940 \n",
        "\n",
        "**\"If you want a MAP estimate for alpha, you need to pyro.sample it from a prior distribution in your model and delta distribution in your guide just like your second guide does with theta, rather than treating it as a constant. If you instead want a maximum likelihood estimate of alpha, you need to wrap it with a pyro.param call in the model and omit it from your guide\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxRoROT_4KzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch.distributions import constraints\n",
        "import torchvision.datasets as dset\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "# Pyro\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, Trace_ELBO,TraceGraph_ELBO,JitTrace_ELBO\n",
        "from pyro.optim import Adam\n",
        "pyro.enable_validation(True)    # <---- This is always a good idea!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define our data and train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Df8qZqslAoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "for _ in range(6):\n",
        "    data.append(torch.tensor(1.0))\n",
        "for _ in range(4):\n",
        "    data.append(torch.tensor(0.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bu232xqk_i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, guide,lr=0.005):\n",
        "  # set up the optimizer\n",
        "  adam_params = {\"lr\": lr, \"betas\": (0.90, 0.999)}\n",
        "  optimizer = Adam(adam_params)\n",
        "  pyro.clear_param_store()\n",
        "  # setup the inference algorithm\n",
        "  svi = SVI(model, guide, optimizer, loss=JitTrace_ELBO())\n",
        "\n",
        "  n_steps = 5000\n",
        "  # do gradient steps\n",
        "  for step in range(n_steps):\n",
        "      loss = svi.step(torch.tensor(data))\n",
        "      if step % 1000 == 0:\n",
        "          print('%.4f' %(loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WSYgrdFjczI",
        "colab_type": "text"
      },
      "source": [
        "# MLE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlny9Usv4PQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_MLE(data):\n",
        "    # define the hyperparameters that control the beta prior\n",
        "\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.param(\"latent_fairness\", torch.tensor(0.1), constraint=constraints.positive)\n",
        "    # loop over the observed data\n",
        "    with pyro.plate('data', len(data)):\n",
        "        # observe datapoint i using the bernoulli\n",
        "        # likelihood Bernoulli(f)\n",
        "        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5th7WXQm4yGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def guide_MLE(data):\n",
        "  # mu_map = pyro.param('mu_map', torch.tensor(1.0))\n",
        "  # pyro.sample('theta', dist.Delta(mu_map))\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGHOjdlq49K2",
        "colab_type": "code",
        "outputId": "292e7629-6981-4888-c9b5-59501098f4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train(model_MLE,guide_MLE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14.2370\n",
            "6.7301\n",
            "6.7301\n",
            "6.7301\n",
            "6.7301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA1OzZ7W5B0T",
        "colab_type": "code",
        "outputId": "8e84a4b2-3ceb-4381-fd10-179ca0cadb1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pyro.param('latent_fairness').item()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5999999642372131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ue5i6V_mAth",
        "colab_type": "text"
      },
      "source": [
        "0.599999 very close to 0.6. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri9W1zy3kZ0U",
        "colab_type": "text"
      },
      "source": [
        "# Posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B21gVtrmaro",
        "colab_type": "text"
      },
      "source": [
        "This is nothing special, I copy whole from the tutorial. Prior is Beta(10,10), then the posterior is 0.533333"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbKgIMjR5Tla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_posterior(data):\n",
        "    # define the hyperparameters that control the beta prior\n",
        "    alpha0 = torch.tensor(10.0)\n",
        "    beta0 = torch.tensor(10.0)\n",
        "    # sample f from the beta prior\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data\n",
        "    with pyro.plate('data', len(data)):\n",
        "        # observe datapoint i using the bernoulli\n",
        "        # likelihood Bernoulli(f)\n",
        "        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n",
        "\n",
        "def guide_posterior(data):\n",
        "    # register the two variational parameters with Pyro\n",
        "    # - both parameters will have initial value 15.0.\n",
        "    # - because we invoke constraints.positive, the optimizer\n",
        "    # will take gradients on the unconstrained parameters\n",
        "    # (which are related to the constrained parameters by a log)\n",
        "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
        "                         constraint=constraints.positive)\n",
        "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
        "                        constraint=constraints.positive)\n",
        "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
        "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KVdFepcmpat",
        "colab_type": "code",
        "outputId": "672b1ab1-6b7d-4507-ea22-be8a2917d070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "train(model_posterior,guide_posterior,lr=0.0005)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7.2614\n",
            "7.0590\n",
            "7.0529\n",
            "7.0668\n",
            "7.0819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMSsweKNmuIQ",
        "colab_type": "code",
        "outputId": "bf308721-5984-452e-dfda-4551321b3ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import math\n",
        "# grab the learned variational parameters\n",
        "alpha_q = pyro.param(\"alpha_q\").item()\n",
        "beta_q = pyro.param(\"beta_q\").item()\n",
        "\n",
        "# here we use some facts about the beta distribution\n",
        "# compute the inferred mean of the coin's fairness\n",
        "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
        "# compute inferred standard deviation\n",
        "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
        "inferred_std = inferred_mean * math.sqrt(factor)\n",
        "\n",
        "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
        "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "based on the data and our prior belief, the fairness of the coin is 0.536 +- 0.090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M98SZEk7vKiD",
        "colab_type": "text"
      },
      "source": [
        "MAP is the mode of posterior distribution, now or we plot the distribution, or we compute it directly. I choose latter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVN3oRfFvHz_",
        "colab_type": "code",
        "outputId": "07165b57-9273-403c-d6ef-2960a80e4c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math\n",
        "# grab the learned variational parameters\n",
        "alpha_q = pyro.param(\"alpha_q\").item()\n",
        "beta_q = pyro.param(\"beta_q\").item()\n",
        "print((alpha_q-1)/(alpha_q+beta_q-2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5386844444728947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0lK1EIpnMzJ",
        "colab_type": "text"
      },
      "source": [
        "# MAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0jcdExQxta1",
        "colab_type": "text"
      },
      "source": [
        "When define Delta distribution, please don't init it as 1 for Beta, it will jump to NaN or inf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAfxFbAxmydO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_map(data):\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(1, 1))\n",
        "    with pyro.plate('data', len(data)):\n",
        "        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n",
        "\n",
        "def guide_map(data):\n",
        "    mu_map = pyro.param(\"mu_map\", torch.tensor(0.5),\n",
        "                         constraint=constraints.positive)\n",
        "    return pyro.sample(\"latent_fairness\", dist.Delta(mu_map))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx5UoDLQns53",
        "colab_type": "code",
        "outputId": "d5c61eab-c983-4a2e-f328-bc44f8bae546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train(model_map,guide_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.9315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6.7301\n",
            "6.7301\n",
            "6.7301\n",
            "6.7301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wzyv0RynzaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_final = guide_map(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcqhkJbMrBJr",
        "colab_type": "code",
        "outputId": "51ffc09b-1fe8-49e8-c57f-df5a6f9277e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "map_final"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6000, grad_fn=<ExpandBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2Vv_Hnu8xO",
        "colab_type": "text"
      },
      "source": [
        "Ok beta(1,1) is uniform, then map equal to mle. = 0.6 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voqjtLasrFUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_map(data):\n",
        "    f = pyro.sample(\"latent_fairness\", dist.Beta(10, 10))\n",
        "    with pyro.plate('data', len(data)):\n",
        "        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n",
        "\n",
        "def guide_map(data):\n",
        "    mu_map = pyro.param(\"mu_map\", torch.tensor(0.5),\n",
        "                         constraint=constraints.positive)\n",
        "    return pyro.sample(\"latent_fairness\", dist.Delta(mu_map))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_dIkWk8vDdr",
        "colab_type": "code",
        "outputId": "243fc836-e2a7-4624-a24e-937999e9fbf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train(model_map,guide_map)\n",
        "map_final = guide_map(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5.6719\n",
            "5.6004\n",
            "5.6004\n",
            "5.6004\n",
            "5.6004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8qOA4HWvHOz",
        "colab_type": "code",
        "outputId": "193ccd8b-c3b4-4fd3-c565-b45e9f2610bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "map_final"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5357, grad_fn=<ExpandBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc6BLZZWhPSc",
        "colab_type": "text"
      },
      "source": [
        "MAP of Beta(10,10) is same as above, great."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOrdNvhMwIgk",
        "colab_type": "text"
      },
      "source": [
        "What is the real MAP if we compute using our knowledge?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1m-8tkOvzj6",
        "colab_type": "code",
        "outputId": "08a027d5-f939-4270-ebac-1e3b5711a9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_alpha = 10 + 6\n",
        "new_beta = 10 + 4\n",
        "print((new_alpha-1)/(new_alpha+new_beta-2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5357142857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGgG5oI2xRyf",
        "colab_type": "text"
      },
      "source": [
        "You see, using Delta this time is more accurate than compute whole posterios distribution. Very accurate indeed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRtXhjVnw8fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}